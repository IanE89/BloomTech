{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TwAP2k_I_41n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OomlVN7zIILb"
      },
      "source": [
        "# Hyperparameter Tuning for Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4L97Zu5IILb"
      },
      "source": [
        "## Learning Objectives\n",
        "* <a href=\"#p1\">Part 1</a>: Describe the major hyperparameters to tune\n",
        "* <a href=\"#p2\">Part 2</a>: Implement an experiment tracking framework\n",
        "* <a href=\"#p3\">Part 3</a>: Search the hyperparameter space using RandomSearch (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toTqYhQvIILf"
      },
      "source": [
        "# 1. Hyperparameter Options (Learn)\n",
        "<a id=\"p1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI3U8thwIILg"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Hyperparameter tuning is much more important with neural networks than it has been with any other models that we have considered up to this point. Other supervised learning models might have a couple of parameters, but neural networks can have dozens. These can substantially affect the accuracy of our models and although it can be a time consuming process is a necessary step when working with neural networks.\n",
        "â€‹\n",
        "Hyperparameter tuning comes with a challenge. How can we compare models specified with different hyperparameters if our model's final error metric can vary somewhat erratically? How do we avoid just getting unlucky and selecting the wrong hyperparameter? This is a problem that to a certain degree we just have to live with as we test and test again. However, we can minimize it somewhat by pairing our experiments with Cross Validation to reduce the variance of our final accuracy values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzBkvEq-z-hb"
      },
      "source": [
        "from keras.activations import relu, sigmoid, softmax, tanh, selu, elu\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s0o2pqBs88q"
      },
      "source": [
        "### Load MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMcNwZDoIILh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe46c4c-4409-4ac3-c131-07d1e6d6c4e9"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "###BEGIN SOLUTION\n",
        "# rescale our pixel values between 0 and 1\n",
        "max_pixel_value = 255\n",
        "X_train = X_train / max_pixel_value\n",
        "X_test = X_test / max_pixel_value\n",
        "\n",
        "# flatten images into row vectors\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "###END SOLUTION"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm7zow5IvaTt"
      },
      "source": [
        "### Normalizing Input Data\n",
        "\n",
        "Recall from our lesson on Gradient Descent, that we need to normalize our input data so that the weights will be updated in equal proportions.\n",
        "\n",
        "**Hint:** if your dataset's values range accross multiple orders of magnitude (i.e. $10^1,~~10^2,~~10^3,~~10^4$), then gradient descent will update the weights in grossly uneven proportions.  \n",
        "\n",
        "\n",
        "![](https://quicktomaster.com/wp-content/uploads/2020/08/contour_plot.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYJ8t_ezHP4W"
      },
      "source": [
        "##1.1 Hyperparameter Tuning Approaches:\n",
        "For an excellent brief article introducing this subject, see [Comparison of Hyperparameter Tuning algorithms: Grid search, Random search, Bayesian optimization](https://medium.com/analytics-vidhya/comparison-of-hyperparameter-tuning-algorithms-grid-search-random-search-bayesian-optimization-5326aaef1bd1)\n",
        "\n",
        "### 1.1.1 Babysitting AKA \"Student Descent\".\n",
        "\n",
        "If you fiddled with any hyperparameters yesterday, this is basically what you did. This approach is 100% manual and is pretty common among researchers, where finding that one exact specification of hyperparameter values that jumps your model to a level of accuracy never seen before is the difference between publishing and not publishing a paper. Of course the professors don't do this themselves, that's grunt work. This is also known as the \"fiddle with hyperparameters until you run out of time\" method.\n",
        "\n",
        "### 1.1.2 Grid Search\n",
        "\n",
        "Grid Search is the Grad Student galaxy brain realization of: why don't I just specify all the experiments I want to run and let the computer try every possible combination of them while I go and grab lunch. This has a specific downside in that if I specify 5 hyperparameters with 5 options each then I've just created $5^5 = 3125$ combinations of hyperparameters to check -- which means that I have to train $3125$ different versions of my model. Then if I use $5\\text{-fold}$ Cross Validation, I need five times as many runs, or $15,625$ runs. This is the brute-force method of hyperparameter tuning, but it can be very profitable if done wisely.\n",
        "\n",
        "### 1.1.3 Random Search\n",
        "\n",
        "Do Grid Search for a couple of hours and you'll say to yourself - \"There's got to be a better way.\" <br>\n",
        "Enter Random Search. For Random search you specify an interval of values to search for each hyperparameter and the search algorithm randomly samples hyperparameters from the specified intervals, and and returns you the best results.\n",
        "\n",
        "The downside of Random search is that it won't find the absolute best hyperparameters, but it is much less costly to perform than Grid Search.\n",
        "\n",
        "### 1.1.4 Bayesian Search\n",
        "\n",
        "One thing that can make manual search methods like babysitting and gridsearch effective is that as the experimenter sees results they can then make updates to their future searches taking into account the previous results. If only we could hyperparameter tune our hyperparameter tuning! <br><br>\n",
        "Well, we can if we use Bayesian Optimization. Tuning Neural Network hyperparameters is like an optimization problem within an optimization problem, and Bayesian Optimization is a search strategy that takes into account the results of past searches in order to improve future ones. <br><br>\n",
        "Bayesian Optimization figures out the most promising regions of hyperparameter space to focus on, so it wastes less time searching through hyperparameter values that are unlikely to lead to improvement.<br><br>\n",
        "Check out the library `keras-tuner` for easy implementations of Bayesian methods.\n",
        "\n",
        "If the Bayesian hyperparameter search strategy piques your interest, here's a nice reference article: <br>\n",
        "[A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfQ7D043OMMn"
      },
      "source": [
        "## 1.2 What Hyperparameters are there to tune?\n",
        "\n",
        "- learning rate\n",
        "- batch_size\n",
        "- number of  training epochs\n",
        "- dropout regularization\n",
        "- number of hidden layers\n",
        "- number of neurons in each hidden layer\n",
        "- optimization algorithms\n",
        "- activation functions\n",
        "- loss functions\n",
        "\n",
        "There are more, but these are the most important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKcuY6OiaLfz"
      },
      "source": [
        "### 1.2.1 Optimizer\n",
        "\n",
        "Optimizers can also be considered as hyperparameters! <br>\n",
        "There are a variety of [**optimizers**](https://keras.io/optimizers/) in Keras, and you may want to include several choices in your hyperparameter tuning process.\n",
        "\n",
        "At some point, take some time to read this article [**An overview of gradient descent optimization algorithms**](https://ruder.io/optimizing-gradient-descent/) by Sebastian Ruder.\n",
        "\n",
        "The **adam** optimizer usually gives great results, so it's your go-to optimizer. Different optimizers have different hyperparameters (such as learning rate, momentum, etc.) So, based on the optimizer you choose you might also have to tune the learning rate and momentum of those optimizers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3wq5iOaLig"
      },
      "source": [
        "### 1.2.2 Learning Rate\n",
        "\n",
        "The Learning Rate is a hyperparameter that is specific to your gradient-descent based optimizer selection. A learning rate that is too high will cause divergent behavior, but a Learning Rate that is too low will fail to converge; you're looking for the sweet spot. Start out tuning learning rates by orders of magnitude: $[0.001, 0.01, 0.1, 0.2, 0.3, 0.5]$ etc.\n",
        "\n",
        "Once you have narrowed down your search, make the window even smaller and try again. If after running the above specification your model reports that $0.1$ is the Learning Rate, then you can bracket $0.1$ by a few other values on either side, say $[0.05, 0.08, 0.1, 0.12, 0.15]$ to try and narrow it down further. If on the other hand, you find that $0.001$ (or $0.5$) is the best Learning Rate, you definitely need to try more values to the left of $0.001$ (or to the right of $0.5$) in order to explore whether lower (or higher) learning rates could perform even better.\n",
        "\n",
        "It can also be good to tune the number of epochs in combination with the learning rate since lower learning rates may need more epochs to converge to the minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNTBUWd1aLlA"
      },
      "source": [
        "### 1.2.3 Momentum\n",
        "\n",
        "**Momentum** is a variation of Stochastic Gradient Descent optimization. SGD is a common optimizer because it's what people understand and know, but it won't always get the best results. You can try adding the momentum option and tuning its hyperparameters to see if you can beat the performance from **Adam**. <br>\n",
        "In SGD with Momentum, parameter updates are adaptive to prevent overshooting the loss function's minimum. Imagine a ball rolling down one side of a bowl, speeding up and rolling past the bottom and up the other side. Momentum stabilizes parameter updates by making them depend partly on the past. If our \"marble\" encounters a rapidly varying region of the loss function, momentum will prevent the parameter updates from changing rapidly in response and therefore overshooting the minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnEG-bCJaLnZ"
      },
      "source": [
        "### 1.2.4  Activation Functions\n",
        "\n",
        "Typically you would choose the **ReLU** activation function for hidden layers. For output layers of binary and multi-class classification models, you would choose the **Sigmoid**, or **Softmax** activations respectively. <br>\n",
        "\n",
        "Be aware that there are [other activation functions available in Keras](https://keras.io/api/layers/activations/) that can potentially improve your results!<br>   \n",
        "For a brief introduction to some alternate activation functions, read [7 popular activation functions you should know in Deep Learning and how to use them with Keras and TensorFlow 2](https://towardsdatascience.com/7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6). Some of these activation functions (such as PReLU -- Parameteric Leaky ReLU) have learnable parameters.<br><br>\n",
        "The choice of activation function is a hyperparameter whose exploration can potentially pay off in the form of better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS68GBrNz-hb"
      },
      "source": [
        "### **Sigmoid**\n",
        "\n",
        "![](https://i.stack.imgur.com/inMoa.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOKc6gUQz-hc"
      },
      "source": [
        "### **ReLU and variants**\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/max/2050/1*ypsvQH7kvtI2BhzR2eT_Sw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acy2j2aVhehc"
      },
      "source": [
        "### Can you code up the ReLU and leaky ReLU functions?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3ef60456ddd99399853561b5e9690d1f",
          "grade": false,
          "grade_id": "cell-113a5f384f5fa8f9",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "bkmtN1L8z-hc"
      },
      "source": [
        "# ReLU: YOUR CODE HERE\n",
        "def relu(x):\n",
        "  if(x>=0):\n",
        "    y = x\n",
        "  else:\n",
        "    y = 0\n",
        "  return y"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3ef60456ddd99399853561b5e9690d1f",
          "grade": false,
          "grade_id": "cell-113a5f384f5fa8f9",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "NjxYsFfQhtPX"
      },
      "source": [
        "# Leaky ReLU: YOUR CODE HERE\n",
        "def leaky_relu(x):\n",
        "  if(x>=0):\n",
        "    y = x\n",
        "  else:\n",
        "    y = 0.01*x\n",
        "  return y"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZV4mvScz-hc"
      },
      "source": [
        "### **Softmax**\n",
        "The main use of softmax activation in neural nets is to map scores to probabilities\n",
        "<br><br><br><br>\n",
        "![](https://miro.medium.com/max/1906/1*ReYpdIZ3ZSAPb2W8cJpkBg.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VPUPQ-2lJsT"
      },
      "source": [
        "The output layer is a vector of numbers ${z_i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-sk-pngz-hc"
      },
      "source": [
        "# Suppose we have a set of K = 5 ourputs z_i\n",
        "outputs = np.array([1.3, 5.1, 2.2, 0.7, 1.1])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo8_Ri9glXm3"
      },
      "source": [
        "Transform the outputs to ${\\exp{z_i}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbUxV6bZz-hd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9e8f06-af80-4534-e2ea-3d656c37f55e"
      },
      "source": [
        "np.exp(outputs).round(3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  3.669, 164.022,   9.025,   2.014,   3.004])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD0iBnQImARb"
      },
      "source": [
        "The ratio $\\frac{{\\exp{z_i}}}{\\sum_{j=1}^{5}\\exp{z_j}}$ is the probability of the $ith$ class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrvSVPnhz-hd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b415828-91fd-4924-db68-8e726d28ccd0"
      },
      "source": [
        "(np.exp(outputs) / np.exp(outputs).sum()).round(2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.02, 0.9 , 0.05, 0.01, 0.02])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxGfjQoQz-he",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6524f9-aa82-427a-ece6-e2b281e1eed1"
      },
      "source": [
        "# implement softmax\n",
        "def softmax(scores):\n",
        "    return (np.exp(scores) / np.exp(scores).sum())\n",
        "\n",
        "softmax(np.array([1.3, 5.1, 2.2, 0.7, 1.1])).round(2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.02, 0.9 , 0.05, 0.01, 0.02])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvXoMMMDz-he"
      },
      "source": [
        "#### Softmax transforms a list of numbers into probabilites.\n",
        "Softmax applies the standard exponential function to each element $z_{i}$ of the input vector $\\mathbf {z}$ <br> and normalizes these values by dividing by the sum of the exponentials. <br><br>\n",
        "\n",
        "\n",
        "Exponentiation maps the real line onto the positive half line.<br>\n",
        "Normalization maps the positive half-line onto the unit interval $[0,1]$ <br>\n",
        "Normalization also ensures that the sum of the components of the output vector ${\\text{softmax} (\\mathbf {z} )}$ is $1$.<br><br>\n",
        "\n",
        "Because they satisfy these properties, transformed output value is a probability:<br>\n",
        "\n",
        "$$ \\text{Probability} = \\frac{\\text{part}}{\\text{whole}}$$<br><br>\n",
        "The $ith$ component of the Softmax mapping is the predicted probability for the $ith$ class.\n",
        "${\\displaystyle \\text{Softmax} (\\mathbf {z} )_{i}={\\frac {e^{z_{i}}}{\\sum _{j=1}^{K}e^{z_{j}}}}{\\text{ for }}i=1,\\dotsc ,K{\\text{ and }}\\mathbf {z} =(z_{1},\\dotsc ,z_{K})\\in \\mathbb {R} ^{K}}$\n",
        "\n",
        "\n",
        "**Take Away:** Softmax is a multi-dimensional generalization of the Sigmoid. In a binary classification problem, the Sigmoid calculates the probability for a class. The probability can be converted to a class prediction by application of a threshold value, or by choosing the class with the highest probability, as we have seen previously. <br>\n",
        "\n",
        "The Softmax calculates a probability for each of a set of classes. The predicted class is the class with the highest softmax probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXGl-FL7z-he"
      },
      "source": [
        "### 1.2.5 Cross Entropy Loss functions\n",
        "\n",
        "You also have the freedom to select various types of loss functions. [**Keras has a library of loss functions that you can select from**](https://keras.io/api/losses/probabilistic_losses/#categorical_crossentropy-function). <br><br>\n",
        "A common class of loss functions to use in mult-class classification tasks are the [Cross Entropy Loss functions](https://ztlevi.gitbook.io/ml-101/loss/cross_entropy_loss). <br>\n",
        "They come in several different formulations. Let's take a look at the basic idea that underpins them all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF9kX3qQz-he"
      },
      "source": [
        "Cross-Entropy loss for a data point is defined as the *negative log of the probability that is predicted for the correct class*. <br><br>\n",
        "The overall Cross-Entropy loss for a batch (or mini-batch) of data points is the sum of the Cross-Entropy losses of the individual data points in the batch (or mini-batch).<br><br>\n",
        "For a binary classification problem, the probability of the positive class is predicted using a Sigmoid activation function, while in a mult-class classification problem, class probabilities are predicted using a Softmax activation function.\n",
        "\n",
        "Cross-Entropy Loss is a measure of how close the predicted targets values are to the true target values. Its minimum possible value is zero, indicating that the predictions agree perfectly with the true targets.<br><br>\n",
        "\n",
        "In the usual case of Multi-Class classification, the labels are One-Hot encoded.\n",
        "\n",
        "If the labels are One-Hot encoded, the target vector has only one nonzero element -- the one that corresponds to the correct class.<br><br>\n",
        "#### Example: <br>\n",
        "For a 5-class classification problem, an example that is in class 1 (0-based indexing) has the following One-Hot encoded target vector:\n",
        "\n",
        "$y = [0, 1, 0, 0, 0]$ <br><br>\n",
        "\n",
        "#### Example:  <br>\n",
        "Suppose we have a $K$-class classification problem, and one of the data points is in the $jth$ class. <br>\n",
        "\n",
        "The cross-entropy loss (CE) for this data point is defined as:<br>\n",
        "\n",
        "$ \\text{CE}~=~-\\sum_{i=0}^{K-1}y_i\\log{p_i} = -\\log{p_j}$ <br><br>\n",
        "\n",
        "In the summation, only the term corresponding to the $jth$ (correct) class survives, because $y_j=1$, and all the other $y_i$ are zeros.<br><br>\n",
        "\n",
        "Notice that the closer $p_j$ gets to $1$, the better our model has done in classifying this example and the closer the cross-entropy loss gets to zero.<br><br>\n",
        "\n",
        "Binary Cross-Entropy Loss (BCE) is a special case of Cross-Entropy Loss for the case of $K=2$ (i.e. binary classification): <br>\n",
        "\n",
        "$BCE = -(y \\in {\\text{class} 0})\\log p_0 - (y \\in {\\text{class} 1})\\log p_1$<br><br>\n",
        "\n",
        "Note that an example must be in one of the two classes, so<br><br>\n",
        "$p_0 = 1-p_1$, and <br>\n",
        "\n",
        "\n",
        "$BCE =  { \\begin{cases}-\\log p_0 = -\\log (1-p_1)& {\\text{if }}\\ y \\in {\\text{class0} } \\\\  - \\log p_1& {\\text{if }}\\ y \\in {\\text{class} 1} \\\\ \\end{cases}}$<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TslCULiumTi9"
      },
      "source": [
        "#### Computing the cross-entropy loss function with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "309aff60acdff1edf8578b967c6aefc5",
          "grade": false,
          "grade_id": "cell-3c2d87bff95fdc05",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "xlQK1S8iz-hf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bfdb9c5-4a37-43d1-e786-8824240bce81"
      },
      "source": [
        "# Example: 5 class problem [red, green, blue, orange, yellow]\n",
        "# these are one-hot encoded labels\n",
        "y_true = [[0, 1, 0 , 0, 0], # one-hot encoded label (this example is greeen)\n",
        "          [1, 0, 0, 0, 0]] # one-hot encoded label (this example is red)\n",
        "\n",
        "# these are probabilities calculated by Softmax\n",
        "y_pred = [[0., 1., 0., 0., 0.],\n",
        "          [0.9, 0.02 , 0.05, 0.01, 0.02]]\n",
        "\n",
        "# if labels are one-hot encoded, you use categorical_crossentropy\n",
        "# if the labels are encoded as single digits, use sparse_categorical_crossentropy\n",
        "#   recall that is how we have been doing things\n",
        "loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "CE_loss = loss.numpy().round(3)\n",
        "CE_loss"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.   , 0.105], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFuX7disz-hf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae1db01-9c5e-4d82-8496-37d2b4ca7ae7"
      },
      "source": [
        "y_true = [[0, 1, 0 , 0, 0]]\n",
        "\n",
        "# input into softmax\n",
        "z = tf.constant([[1.3, 5.1, 2.2, 0.7, 1.1]])\n",
        "\n",
        "# same output as in the image above\n",
        "y = softmax(z)\n",
        "y_pred = y.round(decimals=2)\n",
        "y_pred"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02, 0.9 , 0.05, 0.01, 0.02]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a59d4e04d7865994dec4e765b40a4729",
          "grade": false,
          "grade_id": "cell-2b032877af3b7604",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "KP-ohqo1z-hf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc08683-84f4-48c5-cd29-ff008233c454"
      },
      "source": [
        "# pass one-hot encoded labels (y_true) and the softmax probabilities (y_pred) int CE\n",
        "loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "loss.numpy().round(3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.105], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9bf1efd8770e89927a42603b4dcfef16",
          "grade": false,
          "grade_id": "cell-83d1f3aa527d8d9f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "VzRUEFe9z-hg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade75cd8-3a56-49bf-9765-4f91eeb077ce"
      },
      "source": [
        "p1 = 0.9\n",
        "-1 * np.log(p1).round(3)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.105"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oul9sPq-dU-h"
      },
      "source": [
        "### 1.2.6 Network Weight Initialization\n",
        "\n",
        "![](https://i.pinimg.com/originals/89/f9/bd/89f9bddacf547661dfc209d4b31c2c12.png)\n",
        "\n",
        "**Recall** from our **Gradient Descent** lecture, how we initalize our model weights can determine the difference between Gradient Descent converging towards a local minimum or a global minimum!\n",
        "\n",
        "[Keras has documentation on intializer options](https://keras.io/api/layers/initializers/)\n",
        "\n",
        "`init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']`\n",
        "\n",
        "If you wish to dive into a deep analysis of varying weight initializers and their affect on model performance read this article [Hyper-parameters in Action! Part II â€” Weight Initializers](https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404) by Daniel Godoy/\n",
        "\n",
        "**Take Away:** Include a few different weight initalizers in your gridsearch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqtEuxeQaLqE"
      },
      "source": [
        "### 1.2.7 Regularization\n",
        "In general, Regularization methods make your model more robust against overfitting.\n",
        "\n",
        "There are various types of regularization that you can gridsearch for your model as well. <br>\n",
        "This includes **Dropout** and **Weight Constraints** (you might know this second one as **L2 regularization**). <br>\n",
        "\n",
        "We will do a deep dive into both **Dropout** and **Weight Constraints** in the next Module (4), so we'll hold off discussing them until then."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2c5Cv6oaLtO"
      },
      "source": [
        "### 1.2.8 Number of Hidden Layer Neurons\n",
        "\n",
        "Remember that when we only had a single perceptron our model was only able to fit to linearly separable data, but by adding multiple layers and nodes we can build a network that is a powerhouse of fitting nonlinearity in data. The larger the network and the more nodes generally the stronger the network's capacity to fit nonlinear patterns in data. The more nodes and layers the longer it will take to train a network, and higher the probability of overfitting. The larger your network gets the more you'll need dropout regularization or other regularization techniques to keep it in check.\n",
        "\n",
        "Typically, depth (more layers) is more important than width (more nodes) for neural networks. This is part of why Deep Learning is so highly touted. Certain deep learning architectures have truly been huge breakthroughs for certain machine learning tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5uXdgtIz-hX"
      },
      "source": [
        "-----\n",
        "## 1.3 Brute Force Hyperparameter Gridsearch with sklearn's `GridSearchCV` (Learn)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2smXfriNAGn7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "b4966b8c-ac30-4f3d-d162-599cea2d8e38"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "from scikeras.wrappers import KerasClassifier"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'scikeras'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-58247f70c0d3>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscikeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scikeras'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ffc35b73cb67549ec5d991cebd9088cc",
          "grade": false,
          "grade_id": "cell-3d18235542af580b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "oV5m-tA9z-hX"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(units=32, optimizer='adam', activation='sigmoid'):\n",
        "    \"\"\"\"\n",
        "    Returns a compiled keras model\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    units: int\n",
        "        number of neruons/nodes/units to use in each hidden layer\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model: keras object\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units, input_dim=784, activation=activation))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT6o2_AGz-hY"
      },
      "source": [
        "# Create a Scikit-Learn wrapper around our Keras Model\n",
        "# so that they play nicely together\n",
        "# KerasClassifier needs a model creation function that returns a compiled model\n",
        "model = KerasClassifier(build_fn=create_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToXKXIKOMu28"
      },
      "source": [
        "Models wrapped with `KerasClassifier` have different properties and methods,<br>\n",
        "but some are the same. For example `model.summary()` throws an error:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "3mPo3XY-QXKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5dmA6wiMxhC"
      },
      "source": [
        "dir(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvbb7JIhU3ID"
      },
      "source": [
        "### 1.3.1 Create a dictionary with a grid of hyperparameter values to be searched"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwe315wFz-hZ"
      },
      "source": [
        "# define the search grid of hyparameters\n",
        "# note \"units\" is the number of neurons\n",
        "param_grid = {'batch_size': [32],\n",
        "              'epochs': [5],\n",
        "              'units':[64, 128, 512],\n",
        "              'optimizer': ['adam'],\n",
        "              'activation': ['sigmoid', 'relu']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlF8mDGTVnGj"
      },
      "source": [
        "### 1.3.2 Use `GridSearchCV` to perform the hyperparameter grid search\n",
        "as we have done previously in Sprint 1<br>\n",
        "Takes ~$2$ or $3$ min on Colab with GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNF-tGiMz-hZ"
      },
      "source": [
        "%%time\n",
        "# Create Grid Search\n",
        "model = KerasClassifier(build_fn=create_model)\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-2, verbose=2, cv=3)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrcXQxBUIILy"
      },
      "source": [
        "## Challenge\n",
        "You will be expected to tune several hyperparameters in today's module project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIRKRf5WIILy"
      },
      "source": [
        "# 2. Experiment Tracking Frameworks (Learn)\n",
        "<a id=\"p2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTHOpGLpIILz"
      },
      "source": [
        "## Overview\n",
        "\n",
        "You will notice that managing the results of all the experiments you are running becomes challenging. Which set of parameters did the best? Which code did I use? Are my results today different than my results yesterday? Although we work with Jupyter Notebooks, this format is not well suited to logging experimental results. <br><br>\n",
        "Enter experiment tracking frameworks like [Comet.ml](https://comet.ml) and [Weights and Biases](https://wandb.ai/), and [TensorBoard's Hyperparameter Dashboard](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams)!\n",
        "\n",
        "Those tools will help you track your experiments, store the results, and the code associated with those experiments. Experimental results can also be readily visualized to see changes in performance across any metric you care about. Data is sent to the tool as each epoch is completed, so you can also see if your model is converging in real time. Let's check out TensorBoard today."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-Yhf0eCIILz"
      },
      "source": [
        "## 2.1 TensorBoard Hyperparameter Dashboard(Follow Along)\n",
        "To understand the code in this section, <br>\n",
        "read [Hyperparameter Tuning with the HParams Dashboard](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ncbmt-nAIILz"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDfQi_1SPGVQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "import os\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVNYNxYA7UVy"
      },
      "source": [
        "# we are mainly using hp to initialize the ranges of possible hyper-parameter values\n",
        "# use dir(hp) to check its methods, classes and attributes\n",
        "#      you could also type `hp.` in a code cell and hover the cursor over the period\n",
        "dir(hp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieDkdH3kPR75"
      },
      "source": [
        "### 2.1.1 Create Experiment Configuration\n",
        "We are going to experiment with:\n",
        "* Number of units (neurons) in the first dense layer\n",
        "* Learning Rates\n",
        "* Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "m9DCVHodPkgH",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3b21a84ef3f99a9893852c2b545dcf34",
          "grade": false,
          "grade_id": "cell-17ad234ef5d03bd9",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# let's use Hparams Dashboard\n",
        "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16,32]))\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.001,.01))\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhKcfyJGz-hh"
      },
      "source": [
        "METRIC_ACCURACY = 'accuracy'\n",
        "GRID_SEARCH_RESULTS_DIR = 'logs/hparam_tuning'\n",
        "\n",
        "# creating a dir to save/log our gridsearch results for use with tensorboard\n",
        "# this is a \"context manager\" in python\n",
        "with tf.summary.create_file_writer(GRID_SEARCH_RESULTS_DIR).as_default():\n",
        "\n",
        "    hp.hparams_config(\n",
        "        # store h-params and their values  in a list\n",
        "        hparams=[HP_NUM_UNITS, HP_LEARNING_RATE, HP_OPTIMIZER],\n",
        "\n",
        "        # store metrics to score the model\n",
        "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')]\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rhRMwfLQuGJ"
      },
      "source": [
        "### 2.1.2 Adapt the model to accept hyperparameter values from a dictionary `hparams`\n",
        "This function\n",
        "- builds a model with a set of hyperparameter values specified in the `hparams` dictionary\n",
        "- fits the model\n",
        "- evaluates the fitted model on a test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrjKyNiMQ0Dn"
      },
      "source": [
        "def train_test_model(hparams):\n",
        "  # Hyperparameters\n",
        "  # Sequential() Model\n",
        "  # hparams is a standard python dictionary of hyperparameters with keys and values\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "\n",
        "    # 1st layer in model\n",
        "    tf.keras.layers.Dense(hparams[HP_NUM_UNITS],\n",
        "                          activation='relu'),\n",
        "    # output layer\n",
        "    tf.keras.layers.Dense(10,\n",
        "                          activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # get optimizer from param dict\n",
        "    opt_name = hparams[HP_OPTIMIZER]\n",
        "\n",
        "    # get learning_rate for optimizer\n",
        "    lr = hparams[HP_LEARNING_RATE]\n",
        "\n",
        "    # There is a better way to perform the actions that are being done in this block of code\n",
        "    # see https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/deserialize\n",
        "    if opt_name == 'adam':\n",
        "        # import Adam opt object and set learning rate\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        # import sgd opt object and set leanring rate\n",
        "    elif opt_name == 'sgd':\n",
        "        opt = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "    else:\n",
        "        raise ValueError(f\"unexpected optimizer name: {opt_name}\")\n",
        "\n",
        "    model.compile(\n",
        "          optimizer=opt,\n",
        "          loss='sparse_categorical_crossentropy',\n",
        "          metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=2)\n",
        "\n",
        "    _, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bixdT4BSFvx"
      },
      "source": [
        "### 2.1.3 For each run, log an `hparams` summary with the hyperparameter values and final accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajJRJSCDTLOA"
      },
      "source": [
        "def run(run_dir, hparams):\n",
        "\n",
        "    with tf.summary.create_file_writer(run_dir).as_default():\n",
        "        # record the values used in this trial\n",
        "        hp.hparams(hparams)\n",
        "\n",
        "        # call train_test_model to build, train, and score model on parameter values\n",
        "        accuracy = train_test_model(hparams)\n",
        "\n",
        "        # store trained accuracy to file\n",
        "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHXmqBzxAwXb"
      },
      "source": [
        "This is the main method, so start reading code from here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWC0FMCjTQk1",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "session_num = 0\n",
        "for num_units in HP_NUM_UNITS.domain.values:\n",
        "    for learning_rate in (HP_LEARNING_RATE.domain.min_value, HP_LEARNING_RATE.domain.max_value):\n",
        "        for optimizer in HP_OPTIMIZER.domain.values:\n",
        "\n",
        "            # as we loop through all the hyper-param values\n",
        "            #     store each unique combination in the dictionary hparams\n",
        "            hparams = {\n",
        "               HP_NUM_UNITS: num_units,\n",
        "               HP_LEARNING_RATE: learning_rate,\n",
        "               HP_OPTIMIZER: optimizer\n",
        "            }\n",
        "\n",
        "            run_name = f\"run-{session_num}\"\n",
        "            print(f\"--- Starting trial: {run_name}\")\n",
        "            print({h.name: hparams[h] for h in hparams})\n",
        "\n",
        "            # execute the run function, which runs the training of the models\n",
        "            run('logs/hparam_tuning/' + run_name, hparams)\n",
        "            session_num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdWHqy70YVFY"
      },
      "source": [
        "### 2.1.4 Visualize the Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsPNHbYdz-hj"
      },
      "source": [
        "# run tensorboard in the Colab notebook\n",
        "%tensorboard --logdir=logs/hparam_tuning/ --host localhost --port 8088"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFovWNBmIIL4"
      },
      "source": [
        "### 2.1.5 Your Turn\n",
        "\n",
        "Pick a few hyparameters that we *have not* tuned. Using the above code as a template, try changing a few parameters you're interested in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7584VUQTYst8"
      },
      "source": [
        "##YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFwQuQOGIIL6"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "In today's module assignment, you will be expected to use TensorFlow's HParams API along with TensorBoard to implement and visualize results of hyperparameter tuning scenarios for neural network models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUGXzf6TIIL7"
      },
      "source": [
        "# 3. Hyperparameter Tuning with RandomSearchCV (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRu7-zZIIL7"
      },
      "source": [
        "## Overview\n",
        "\n",
        "`GridSearchCV` can take a long time to systematically explore a hyper-parameter search space. You'll want to adopt more sophiscated strategy such as a Random Search.\n",
        "\n",
        "Let's see how to do this with with [`keras-tuner`](https://keras.io/keras_tuner/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDaA4nDJqzwj"
      },
      "source": [
        "## 3.1 Install `keras-tuner`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-tDdeWYIIL7"
      },
      "source": [
        "!pip install keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO5PEiGcIIL9"
      },
      "source": [
        "## 3.2 Perform Random Search with `keras-tuner` (Follow Along)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vemFBZ3XsQ4P"
      },
      "source": [
        "### 3.2.1 Set up the hyperparameter search space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKxI2vx4IIL9"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras_tuner import RandomSearch\n",
        "\n",
        "\"\"\"\n",
        "This model Tunes:\n",
        "- Number of Neurons in the Hidden Layer\n",
        "- Learning Rate in Adam\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Dense(units=hp.Int('units',\n",
        "                                        min_value=32,\n",
        "                                        max_value=512,\n",
        "                                        step=32),\n",
        "                           activation='relu'))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    model.compile(\n",
        "      optimizer=keras.optimizers.Adam(\n",
        "          hp.Choice('learning_rate',\n",
        "                    values=[1e-1, 1e-2, 1e-3])),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv2gEY_Ssc34"
      },
      "source": [
        "### 3.2.2 Set up the `RandomSearch()` tuner\n",
        "to do $5$ trials, each with $3$ sets of hyperparameters randomly chosen from our grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBCfWIweIIL_"
      },
      "source": [
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=3,\n",
        "    directory='./keras-tuner-trial',\n",
        "    project_name='mnist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RziSPNKvl4Vu"
      },
      "source": [
        "#### Check that the search space corresponds with what you ordered"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpGTo9bpIIMB"
      },
      "source": [
        "tuner.search_space_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdmzODK2soQO"
      },
      "source": [
        "### 3.2.3 Run the Random Search\n",
        "use $5$ epochs for each run<br>\n",
        "This takes ~$9$ min on Colab with GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SghuI4TqIIME"
      },
      "source": [
        "%%time\n",
        "tuner.search(X_train, y_train,\n",
        "             epochs=5,\n",
        "             validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbn62iAdsxG6"
      },
      "source": [
        "### 3.2.4 Report the search results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCmme1xjIIMG"
      },
      "source": [
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgVwg4MtIIMI"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "In your module project today, you will apply `RandomSearch` and `BayesianSearch` using `keras-tuner`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-e_aIT8IIMI"
      },
      "source": [
        "# Review\n",
        "* <a href=\"#p1\">Part 1</a>: Describe the major hyperparameters to tune\n",
        "    - Activation Functions\n",
        "    - Optimizer\n",
        "    - Number of Layers\n",
        "    - Number of Neurons\n",
        "    - Batch Size\n",
        "    - Dropout Regulaization\n",
        "    - Learning Rate\n",
        "    - Number of Epochs\n",
        "    - and many more\n",
        "* <a href=\"#p2\">Part 2</a>: Implement an experiment tracking framework\n",
        "    - By Hand: GridSearchCV\n",
        "    - TensorBoard with Hparams Dashboard\n",
        "    - Stretch topic: other experiment tracking frameworks\n",
        "    > [Weights & Biases](https://wandb.ai/site)<br>\n",
        "    > [Comet.ml](https://www.comet.ml/site/)<br>\n",
        "    > [neptune.ai](https://neptune.ai/)<br>\n",
        "\n",
        "* <a href=\"#p3\">Part 3</a>: Search the hyperparameter space using Keras-Tuner\n",
        "    - Random Search\n",
        "    - Bayesian Search\n",
        "    - Stretch Topic: [Advanced Hyperparameter Optimization Techniques](https://neptune.ai/blog/hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms)\n",
        "    > Hyperband<br>\n",
        "    > BOHB (Bayesian Optimization + Hyperband)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7it81sfIIMJ"
      },
      "source": [
        "# Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPkG9C2hIIMJ"
      },
      "source": [
        "## Additional Reading\n",
        "- [How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)\n",
        "- [Practical Guide to Hyperparameters Optimization for Deep Learning Models](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/)\n",
        "- [ML Experiment Tracking: What It Is, Why It Matters, and How to Implement It](https://neptune.ai/blog/ml-experiment-tracking)\n",
        "- [Dropout Regularization in Deep Learning Models With Keras](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)\n",
        "- [A Gentle Introduction to Weight Constraints in Deep Learning](https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/)\n",
        "- [How to Configure the Number of Layers and Nodes in a Neural Network](https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/)\n"
      ]
    }
  ]
}